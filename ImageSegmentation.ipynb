{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageSegmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8w59OT+K3m/MJpsOixBr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erickmu1/Image-Segmentation/blob/E/ImageSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Fje7Mu90f2"
      },
      "source": [
        "# APS360: Image Segmentation #\n",
        "\n",
        "### **Team 5** ###\n",
        "- Bonnie He\n",
        "- Erick Mejia Uzeda\n",
        "- Hannah Lee\n",
        "\n",
        "## Project Description ##\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CUYUzA7EoEN"
      },
      "source": [
        "# Imports + Global Variables #\n",
        "\n",
        "Here we import all required libraries and define any useful variables. (Feel free to improve this description)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAm7Ch3PFQIA"
      },
      "source": [
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data storage/loading\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Global Variables\n",
        "ROOT = '\\\\'\n",
        "BACKGROUND_ID = 0 # TODO: actually determine a unique ID for 'background'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DETWb7mU-3_s"
      },
      "source": [
        "# Data Loading #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERXJz_dd9zKW"
      },
      "source": [
        "# Code for loading data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6dR-H5-_HeH"
      },
      "source": [
        "# Data Pre-processing #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypZeIW3NCP-O"
      },
      "source": [
        "## Pre-computing Features from other ML models ##\n",
        "\n",
        "We may have parts of the full ML pipeline implemented and others not. To speed up the training process, it is beneficial to precompute features resulting from a model and save them.\n",
        "\n",
        "**Note:** Features are saved in such a format that once re-loaded, they can be passed directly into a `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JYyF3N7_QMU"
      },
      "source": [
        "# Compute and Save Features from 'model'\n",
        "def save_features(model, data_loader, file_name, dir=ROOT, use_cuda=False):\n",
        "  features = []\n",
        "\n",
        "  for input, label in data_loader:\n",
        "    # Enable CUDA\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      input = input.cuda()\n",
        "      label = label.cuda()\n",
        "    \n",
        "    # Compute features\n",
        "    with torch.no_grad():\n",
        "      output = model(input)\n",
        "\n",
        "    # Cache resulting features\n",
        "    features.extend(output.cpu())\n",
        "  \n",
        "  # Save computed features using pickle\n",
        "  save_path = os.path.join(dir, file_name + '.pickle')\n",
        "\n",
        "  with open(save_path, 'wb+') as f:\n",
        "    pickle.dump(features, f)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AahfXgL2JUjB"
      },
      "source": [
        "# Load features\n",
        "def load_features(file_name, dir=ROOT):\n",
        "  file_path = os.join.path(dir, file_name + '.pickle')\n",
        "\n",
        "  # Load features using pickle\n",
        "  with open(file_path, 'rb') as f:\n",
        "    features = pickle.load(f)\n",
        "  \n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_BM7bWJLnai"
      },
      "source": [
        "## Grouping Background Segments ##\n",
        "\n",
        "Our dataset has many labels for different categories. Since our goal is to segment non-background items that are *distinct*, we will pre-process the raw segmentation maps to group relevant labels/categories that could be considered as *background*.\n",
        "\n",
        "The following are grouped together: (Background)\n",
        "- `floor`\n",
        "- `wall`\n",
        "- `ceiling`\n",
        "- `window`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IaMKPLZMhul"
      },
      "source": [
        "# Group all 'background' segments into one category\n",
        "def group_background(seg_maps):\n",
        "  # ASSUME: seg_maps is a np.array with dimensions (num_samples x [image_dims])\n",
        "  # NOTE: this function modifies seg_maps itself!\n",
        "\n",
        "  relevant_labels = [ 'floor', 'wall', 'ceiling', 'window' ]\n",
        "  relevant_ids = [ 0, 1, 2, 3 ]  # TODO: get *actual* numerical IDs\n",
        "\n",
        "  # Retrieve indices (pixels) of relevant labels\n",
        "  indices = np.isin(seg_maps, relevant_ids)\n",
        "  \n",
        "  # Associate such indices (pixels) as 'background'\n",
        "  seg_maps[indices] = BACKGROUND_ID"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYww3gwsOjee"
      },
      "source": [
        "## Dealing with Label Permutation and Number of Segments ##\n",
        "\n",
        "Our model will not particularly care what label is associated to segments, hence we prefer to refer to a segment by its segment ID. To re-iterate, our model does not care about what the value of the segment ID is for any given segment, what will dictate if our model is working if it can properly distinguish two **distinct** segments. Thus given an outputted segmentation map, any same segmentation map but with the segment IDs permutted is equally valid. Moreover, our model (in theory) should be invariant to more than permutation, that is any value can be assigned to a segment, as long as distinct segments have distinct IDs.\n",
        "\n",
        "Next, the number of segments depends generally on the number of objects in the scene, so it not known a priori for new samples. This suggests the use of a recurrent architecture but once again, the order in which the segments would be generated is not of importance. In general, dealing with a variable number of possible segments must be accounted for in the model!\n",
        "\n",
        "Ideas:\n",
        "- Use a permutation invariant loss function:https://openreview.net/pdf?id=rJxpuoCqtQ\n",
        "  - A major issue is that the number of segments produced for an image is not fixed, whereas this paper assumes the number of features `F` is known.\n",
        "  - Should we not upper bound the outputs and round each pixel value to the nearest integer as to define its *segment ID*? How sensitive to small weight variantions would our model be? How dependent on the segment ID values temselves will the model be?\n",
        "- Should we use a Recurrent Architecture?\n",
        "  - That is: for each image\n",
        "    1. extract the \"largest\" label\n",
        "    2. remove all pixels from input that correspond to the segment meant to be removed\n",
        "    3. feed in new image and repeat from step 1.\n",
        "  - Note that we will need to either pre-process many images during training (which will make it slow) or cache multiple variants of an image which remove one segment at a time (can grow large if image has many segments)\n",
        "    - Also need to pre-process and \"order\" segments by how \"big\" they are (need to compute pixel area of each segment and order them before making \"inputs\" that have the relevant segments removed)\n",
        "  - We could possibly apply a *Recurrent Pipeline* to our baseline model (k-means) too!\n",
        "- Say we generate all segments from the image in one-pass of the model (simple autoencoder architecture)\n",
        "  - How do we deal with segment ID permutations (and even just different IDs in general, as long as the correct distinct segments have distinct IDs)?\n",
        "  - How do we encode each segment ID?\n",
        "\n",
        "**KEY IDEA:** What if we followed the convention that the largest segment (other than the background) must have the largest (smallest?) ID? Could this additional rule be learned by the model?\n",
        "- If yes, then we resolved the issue of segment ID permutation\n",
        "  - What happens for segments of comparable size? (to think about later)\n",
        "- Could we then also apply *integer thresholding* (or some other thresholding) to identify each distinct segment? This would avoid the need to use a recurrent architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yq65r9pPqPM"
      },
      "source": [
        "# Code (if any) for pre-processing dataset to account for segment ID permutation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEJJLIRPpqz"
      },
      "source": [
        "## Data Augmentation (Optional) ##\n",
        "\n",
        "See if we can apply homographies on RGBD + ground truth segmentation map and if we can add noise to the RGBD images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvnnX-ZvQisd"
      },
      "source": [
        "# Code for homographic transformation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wubp1JkQnwY"
      },
      "source": [
        "# Code for adding noise to {RGB, Depth}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZSROAWI_TCY"
      },
      "source": [
        "# Baseline model: k-means Algorithm #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7E5TBh9_ZaX"
      },
      "source": [
        "# Code for k-means algorithm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdEVNEbg_gAU"
      },
      "source": [
        "# Training Pipeline #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lxwI7pS_o6L"
      },
      "source": [
        "# Code for the training pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzbiJDXb_tya"
      },
      "source": [
        "# Model Implementation #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R14JUk6S_xmJ"
      },
      "source": [
        "# Code for implementing the propsed model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}